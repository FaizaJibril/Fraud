# -*- coding: utf-8 -*-
"""testing top 20 k.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A1QbOxBgGkktFJoiA6UM8l8y1XGg4pY2
"""









# Commented out IPython magic to ensure Python compatibility.
#Import the necessary libraries
from time import time
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from random import randint
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')
# Machine Learning
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
import warnings


from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc

from sklearn.naive_bayes import GaussianNB
import lightgbm as lgb
from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve
from sklearn.metrics import ConfusionMatrixDisplay
import keras
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D

classifiers = ['LinearSVM', 'RadialSVM', 'Logistic', 'RandomForest', 'LightGBM',
               'NaiveBayes','NEURAL NETWORK', 'HYBRID', 'CNN']

models = [
    svm.SVC(kernel='linear'),
    svm.SVC(kernel='rbf'),
    LogisticRegression(max_iter=1000),
    RandomForestClassifier(n_estimators=150, random_state=42),
    lgb.LGBMClassifier(random_state=42),
    GaussianNB(),

]



"""<h3>Transaction Table<h3>
    
<p>TransactionDT: timedelta from a given reference datetime (not an actual timestamp)

TransactionAMT: transaction payment amount in USD

ProductCD: product code, the product for each transaction

card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.

addr: address

dist: distance ("distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.”)
    
P_ and (R__) emaildomain: purchaser and recipient email domain (“ certain transactions don't need recipient, so R_emaildomain is null.”)

C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.(counts of phone numbers, email addresses, names associated with the user plus like device, ipaddr, billingaddr, etc. Also these are for both purchaser and recipient, which doubles the number.)

D1-D15: timedelta, such as days between previous transaction, etc.

M1-M9: match, such as names on card and address, etc.

Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.   <p>

# Exploratory Data Analysis
"""

# Helper function
def reduce_mem_usage(df):
    """ iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.
    """
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))

    for col in df.columns:
        col_type = df[col].dtype

        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))

    return df

train_transaction = pd.read_csv('/content/train_transaction.csv')
print(train_transaction.shape)
train_transaction = reduce_mem_usage(train_transaction)
train_transaction.head()

train_identity = pd.read_csv('/content/train_identity.csv')
print(train_identity.shape)
train_indentity = reduce_mem_usage(train_identity)
train_identity.head()

test_transaction = pd.read_csv('/content/test_transaction.csv')
print(test_transaction.shape)
test_transaction = reduce_mem_usage(test_transaction)
test_transaction.head()

test_identity = pd.read_csv('/content/test_identity.csv')
print(test_identity.shape)
test_indentity = reduce_mem_usage(test_identity)
test_identity.head()

# Merging transaction and identity train data
train_df = pd.merge(train_transaction, train_identity, how='left')
print(train_df.shape)
len_train_df = len(train_df)
del train_transaction, train_identity
train_df.head()

test_df = pd.merge(test_transaction, test_identity, how='left')
test_df.columns = train_df.drop('isFraud', axis=1).columns
print(test_df.shape)
del test_transaction, test_identity
test_df.head()



# Duplicates check in train data
train_df.duplicated().sum()

# Class imbalance check
plt.pie(train_df.isFraud.value_counts(), labels=['Not Fraud', 'Fraud'], autopct='%0.1f%%')
plt.axis('equal')
plt.show()

# Timestamp of train and test data
plt.figure(figsize=(8, 4))
plt.hist(train_df['TransactionDT'], label='Train')
plt.hist(test_df['TransactionDT'], label='Test')
plt.ylabel('Count')
plt.title('Transaction Timestamp')
plt.legend()
plt.tight_layout()
plt.show()

# Missing values check
combined_df = pd.concat([train_df.drop(columns=['isFraud', 'TransactionID']), test_df.drop(columns='TransactionID')])
print(combined_df.shape)

# Dependent variable
y = train_df['isFraud']

print(y.shape)

# Dropping columns with more than 20% missing values
mv = combined_df.isnull().sum()/len(combined_df)
combined_mv_df = combined_df.drop(columns=mv[mv>0.2].index)
print(combined_mv_df.shape)



# Filtering numerical data
num_mv_df = combined_mv_df.select_dtypes(include=np.number)
print(num_mv_df.shape)

# Filtering categorical data
cat_mv_df = combined_mv_df.select_dtypes(exclude=np.number)
print(cat_mv_df.shape)
del combined_mv_df

# Filling missing values by median for numerical columns
imp_median = SimpleImputer(missing_values=np.nan, strategy='median')
num_df = pd.DataFrame(imp_median.fit_transform(num_mv_df), columns=num_mv_df.columns)
del num_mv_df
print(num_df.shape)

# Filling missing values by most frequent value for categorical columns
imp_max = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
cat_df = pd.DataFrame(imp_max.fit_transform(cat_mv_df), columns=cat_mv_df.columns)
del cat_mv_df
print(cat_df.shape)

# Concatinating numerical and categorical data
combined_df_cleaned = pd.concat([num_df, cat_df], axis=1)
del num_df, cat_df

# Verifying missing values
print(f'Total missing values: {combined_df_cleaned.isnull().sum().sum()}')
print(combined_df_cleaned.shape)
combined_df_cleaned.head()

# One-hot encoding
combined_df_encoded = pd.get_dummies(combined_df_cleaned, drop_first=True)
print(combined_df_encoded.shape)
del combined_df_cleaned
combined_df_encoded.head()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.metrics import precision_recall_curve, auc
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.ensemble import RandomForestClassifier

def acc_score(X_train_smote, Y_train_smote, X_val, Y_val, X_test, Y_test, models, classifiers):
    Score = pd.DataFrame({"Classifier": classifiers})
    train_acc = []
    val_acc = []
    test_acc = []
    train_prec = []
    val_prec = []
    test_prec = []
    train_sens = []
    val_sens = []
    test_sens = []
    train_f1 = []
    val_f1 = []
    test_f1 = []
    roc_auc_scores = []
    test_predictions = []

    for i, model in enumerate(models):
        print(f"Training model {i + 1}/{len(models)}...")
        model.fit(X_train_smote, Y_train_smote)
        train_pred = model.predict(X_train_smote)
        val_pred = model.predict(X_val)
        test_pred = model.predict(X_test)

        train_acc.append(accuracy_score(Y_train_smote, train_pred))
        val_acc.append(accuracy_score(Y_val, val_pred))
        test_acc.append(accuracy_score(Y_test, test_pred))

        train_prec.append(precision_score(Y_train_smote, train_pred))
        val_prec.append(precision_score(Y_val, val_pred))
        test_prec.append(precision_score(Y_test, test_pred))

        train_sens.append(recall_score(Y_train_smote, train_pred))
        val_sens.append(recall_score(Y_val, val_pred))
        test_sens.append(recall_score(Y_test, test_pred))

        train_f1.append(f1_score(Y_train_smote, train_pred))
        val_f1.append(f1_score(Y_val, val_pred))
        test_f1.append(f1_score(Y_test, test_pred))

        if hasattr(model, "predict_proba"):
            model.fit(X_train_smote, Y_train_smote)
            fpr, tpr, thresholds = roc_curve(Y_val, model.predict_proba(X_val)[:, 1])
            roc_auc = auc(fpr, tpr)
            roc_auc_scores.append(roc_auc)
        else:
            roc_auc_scores.append(np.nan)

        test_predictions.append(test_pred)
        print(f"Model {i + 1}/{len(models)} training completed.")

    nn_model = Sequential([
        Dense(128, activation='relu', input_shape=(X_train_smote.shape[1],)),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    nn_model.fit(X_train_smote, Y_train_smote, epochs=10, verbose=0)
    nn_test_pred = nn_model.predict(X_test)
    nn_test_pred = (nn_test_pred >= 0.5).astype(int)

    nn_test_acc = accuracy_score(Y_test, nn_test_pred)
    train_acc.append(np.nan)
    val_acc.append(np.nan)
    test_acc.append(nn_test_acc)
    train_prec.append(np.nan)
    val_prec.append(np.nan)
    test_prec.append(precision_score(Y_test, nn_test_pred))
    train_sens.append(np.nan)
    val_sens.append(np.nan)
    test_sens.append(recall_score(Y_test, nn_test_pred))
    train_f1.append(np.nan)
    val_f1.append(np.nan)
    test_f1.append(f1_score(Y_test, nn_test_pred))
    roc_auc_scores.append(np.nan)

    lgb_model = lgb.LGBMClassifier(random_state=42)
    lgb_model.fit(X_train_smote, Y_train_smote)
    hybrid_model = RandomForestClassifier(n_estimators=150, random_state=0)
    hybrid_model.fit(X_train_smote, Y_train_smote)
    hybrid_test_pred = (hybrid_model.predict_proba(X_test)[:, 1] + lgb_model.predict_proba(X_test)[:, 1]) / 2
    hybrid_test_pred = (hybrid_test_pred >= 0.5).astype(int)

    hybrid_test_acc = accuracy_score(Y_test, hybrid_test_pred)
    train_acc.append(np.nan)
    val_acc.append(np.nan)
    test_acc.append(hybrid_test_acc)
    train_prec.append(np.nan)
    val_prec.append(np.nan)
    test_prec.append(precision_score(Y_test, hybrid_test_pred))
    train_sens.append(np.nan)
    val_sens.append(np.nan)
    test_sens.append(recall_score(Y_test, hybrid_test_pred))
    train_f1.append(np.nan)
    val_f1.append(np.nan)
    test_f1.append(f1_score(Y_test, hybrid_test_pred))
    roc_auc_scores.append(np.nan)

    cnn_model = Sequential([
        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_smote.shape[1], 1)),
        MaxPooling1D(pool_size=2),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    cnn_X_test = np.expand_dims(X_test, axis=2)
    cnn_test_pred = cnn_model.predict(cnn_X_test)
    cnn_test_pred = (cnn_test_pred >= 0.5).astype(int)

    cnn_test_acc = accuracy_score(Y_test, cnn_test_pred)
    train_acc.append(np.nan)
    val_acc.append(np.nan)
    test_acc.append(cnn_test_acc)
    train_prec.append(np.nan)
    val_prec.append(np.nan)
    test_prec.append(precision_score(Y_test, cnn_test_pred))
    train_sens.append(np.nan)
    val_sens.append(np.nan)
    test_sens.append(recall_score(Y_test, cnn_test_pred))
    train_f1.append(np.nan)
    val_f1.append(np.nan)
    test_f1.append(f1_score(Y_test, cnn_test_pred))
    roc_auc_scores.append(np.nan)

    # Make sure all lists have the same length
    length = len(classifiers)
    train_acc = ensure_length(train_acc, length)
    val_acc = ensure_length(val_acc, length)
    test_acc = ensure_length(test_acc, length)
    train_prec = ensure_length(train_prec, length)
    val_prec = ensure_length(val_prec, length)
    test_prec = ensure_length(test_prec, length)
    train_sens = ensure_length(train_sens, length)
    val_sens = ensure_length(val_sens, length)
    test_sens = ensure_length(test_sens, length)
    train_f1 = ensure_length(train_f1, length)
    val_f1 = ensure_length(val_f1, length)
    test_f1 = ensure_length(test_f1, length)
    roc_auc_scores = ensure_length(roc_auc_scores, length)

    Score["Training Accuracy"] = train_acc
    Score["Validation Accuracy"] = val_acc
    Score["Test Accuracy"] = test_acc
    Score["Training Precision"] = train_prec
    Score["Validation Precision"] = val_prec
    Score["Test Precision"] = test_prec
    Score["Training Sensitivity"] = train_sens
    Score["Validation Sensitivity"] = val_sens
    Score["Test Sensitivity"] = test_sens
    Score["Training F1-Score"] = train_f1
    Score["Validation F1-Score"] = val_f1
    Score["Test F1-Score"] = test_f1
    Score["ROC AUC"] = roc_auc_scores

    Score.sort_values(by="Validation Accuracy", ascending=False, inplace=True)
    Score.reset_index(drop=True, inplace=True)

    return Score, test_predictions

def ensure_length(lst, length):
    if len(lst) == length:
        return lst
    elif len(lst) == 0:
        return [np.nan] * length
    else:
        return lst + [np.nan] * (length - len(lst))


def plot(score, x, y, c="b"):
    gen = [1, 2, 3, 4, 5]
    plt.figure(figsize=(6, 4))
    ax = sns.pointplot(x=gen, y=score, color=c)
    ax.set(xlabel="Generation", ylabel="Accuracy")
    ax.set(ylim=(x, y))

def initilization_of_population(size, n_feat):
    population = []
    for i in range(size):
        chromosome = np.ones(n_feat, dtype=np.bool)
        chromosome[:int(0.3 * n_feat)] = False
        np.random.shuffle(chromosome)
        population.append(chromosome)
    return population

def fitness_score(population, X_train_Smote_subset2, y_train_smote_subset2, X_val_scaled, Y_val, X_test_scaled, Y_test, model, num_features=20):
    scores = []
    recalls = []
    precisions = []
    sensitivities = []
    auc_rocs = []
    pr_aucs = []  # List to store precision-recall AUC values
    test_accs = []  # List to store test accuracies
    pop_after_fit = []

    for chromosome in population:
        top_features = select_top_features(chromosome, num_features)

        # Train the model using the oversampled training set and selected top features
        model.fit(X_train_Smote_subset2[:, top_features], y_train_smote_subset2)

        # Evaluate the model on the validation set using the same selected features
        y_pred_proba = model.predict_proba(X_val_scaled[:, top_features])[:, 1]
        y_pred = (y_pred_proba >= 0.98).astype(int)

        score = model.score(X_val_scaled[:, top_features], Y_val)
        recall = recall_score(Y_val, y_pred)
        precision = precision_score(Y_val, y_pred)
        sensitivity = recall  # Sensitivity is the same as recall

        fpr, tpr, _ = roc_curve(Y_val, y_pred_proba)
        auc_roc = roc_auc_score(Y_val, y_pred_proba)

        # Calculate precision-recall curve and AUC
        precision_curve, recall_curve, _ = precision_recall_curve(Y_val, y_pred_proba)
        pr_auc = auc(recall_curve, precision_curve)

        # Calculate test accuracy using the same selected features
        y_pred_test = model.predict(X_test_scaled[:, top_features])
        test_acc = accuracy_score(Y_test, y_pred_test)

        scores.append(score)
        recalls.append(recall)
        precisions.append(precision)
        sensitivities.append(sensitivity)
        auc_rocs.append(auc_roc)
        pr_aucs.append(pr_auc)
        test_accs.append(test_acc)
        pop_after_fit.append(chromosome)

    return scores, recalls, precisions, sensitivities, auc_rocs, pr_aucs, test_accs, pop_after_fit


def select_top_features(chromosome, num_features=177):
    # Replace this with the logic to extract the top features from the chromosome
    top_features = np.where(chromosome)[0][:num_features]
    return top_features


def selection(pop_after_fit, n_parents):
    population_nextgen = []
    for i in range(n_parents):
        population_nextgen.append(pop_after_fit[i])
    return population_nextgen


def crossover(pop_after_sel):
    pop_nextgen = pop_after_sel
    for i in range(0, len(pop_after_sel), 2):
        new_par = []
        child_1, child_2 = pop_nextgen[i], pop_nextgen[i + 1]
        new_par = np.concatenate((child_1[:len(child_1) // 2], child_2[len(child_1) // 2:]))
        pop_nextgen.append(new_par)
    return pop_nextgen


def mutation(pop_after_cross, mutation_rate, n_feat):
    mutation_range = int(mutation_rate * n_feat)
    pop_next_gen = []
    for n in range(0, len(pop_after_cross)):
        chromo = pop_after_cross[n]
        rand_posi = []
        for i in range(0, mutation_range):
            pos = np.random.randint(0, n_feat - 1)
            rand_posi.append(pos)
        for j in rand_posi:
            chromo[j] = not chromo[j]
        pop_next_gen.append(chromo)
    return pop_next_gen


def generations(X_train_Smote_subset2, y_train_smote_subset2, X_val_scaled, y_val, X_test_scaled, y_test, size, n_feat, n_parents, mutation_rate, n_gen, model):
    population_nextgen = initilization_of_population(size, n_feat)
    best_chromosome = None
    best_score = []
    best_recalls = []
    best_precisions = []
    best_f1_scores = []  # List to store best F1 scores
    best_auc_rocs = []
    best_pr_aucs = []  # List to store best precision-recall AUC values
    best_test_accs = []  # List to store best test accuracies

    for i in range(n_gen):
        scores, recalls, precisions, f1_scores, auc_rocs, pr_aucs, test_accs, pop_after_fit = fitness_score(population_nextgen, X_train_Smote_subset2, y_train_smote_subset2, X_val_scaled, y_val, X_test_scaled, y_test, model)
        print('Best score in generation', i + 1, ':', scores[:1])  # Print the best score
        pop_after_sel = selection(pop_after_fit, n_parents)
        pop_after_cross = crossover(pop_after_sel)
        population_nextgen = mutation(pop_after_cross, mutation_rate, n_feat)

        # Store the best chromosome and its scores
        best_chromosome = pop_after_fit[0]
        best_score.append(scores[0])
        best_recalls.append(recalls[0])
        best_precisions.append(precisions[0])
        best_f1_scores.append(f1_scores[0])
        best_auc_rocs.append(auc_rocs[0])
        best_pr_aucs.append(pr_aucs[0])
        best_test_accs.append(test_accs[0])  # Store the best test accuracy

    # Calculate FPR and TPR for the best chromosome
    best_model = RandomForestClassifier(random_state=42)
    best_model.fit(X_train_Smote_subset2[:, best_chromosome], y_train_smote_subset2)
    y_pred_proba = best_model.predict_proba(X_val_scaled[:, best_chromosome])[:, 1]
    precision_curve, recall_curve, _ = precision_recall_curve(y_val, y_pred_proba)
    pr_auc = auc(recall_curve, precision_curve)
    fpr, tpr, _ = roc_curve(y_val, y_pred_proba)

    # Plot ROC AUC curve and display confusion matrix using test dataset
    plt.figure(figsize=(12, 5))

    # ROC AUC curve
    plt.subplot(1, 3, 1)
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % best_auc_rocs[-1])
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")

    # Precision-Recall curve
    plt.subplot(1, 3, 2)
    plt.plot(recall_curve, precision_curve, color='darkorange', lw=2, label='Precision-Recall curve (area = %0.2f)' % best_pr_aucs[0])
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    plt.title('Precision-Recall Curve')
    plt.legend(loc="lower right")

    # Confusion matrix
    plt.subplot(1, 3, 3)
    y_pred = best_model.predict(X_test_scaled[:, best_chromosome])
    cm_display = ConfusionMatrixDisplay.from_estimator(best_model, X_test_scaled[:, best_chromosome], y_test)
    cm_display.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')

    plt.tight_layout()
    plt.show()

    return best_score, best_recalls, best_precisions, best_f1_scores, best_auc_rocs, best_pr_aucs, best_test_accs, best_chromosome





label_pd= train_df['isFraud']

X = combined_df_encoded.iloc[:len_train_df]
print(X.shape)
test = combined_df_encoded.iloc[len_train_df:]
print(test.shape)
combined_df_encoded

# Time-based train validation splitting with 20% data in validation set
train = pd.concat([X, y], axis=1)
train.sort_values('TransactionDT', inplace=True)
X = train.drop(['isFraud'], axis=1)
y = train['isFraud']
splitting_index = int(0.7*len(X))
X_train = X.iloc[:splitting_index].values
X_val = X.iloc[splitting_index:].values
y_train = y.iloc[:splitting_index].values
y_val = y.iloc[splitting_index:].values
X_test = X.values
y_test = y.values
print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)
del y, train

# Standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)
del X_train, X_val, test

# Class imbalance check
pd.value_counts(y_train)



from sklearn.decomposition import PCA
from sklearn.metrics import precision_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

# Perform PCA to reduce dimensions
pca = PCA(n_components=0.50)  # Retain 95% of the variance
X_train_pca = pca.fit_transform(X_train_scaled)
X_val_pca = pca.transform(X_val_scaled)
X_test_pca = pca.transform(X_test_scaled)


# Reduce dataset size further by selecting a random subset
subset_size = min(2000, X_val_pca.shape[0])  # Choose a subset size that is within the bounds
# Generate random indices for subsetting
train_subset_indices = np.random.choice(X_train_pca.shape[0], subset_size, replace=False)
val_subset_indices = np.random.choice(X_val_pca.shape[0], subset_size, replace=False)
test_subset_indices = np.random.choice(X_test_pca.shape[0], subset_size, replace=False)

# Subset the datasets using the generated indices
X_train_pca_subset = X_train_pca[train_subset_indices]
y_train_smote_subset = y_train[train_subset_indices]

X_val_pca_subset = X_val_pca[val_subset_indices]
y_val_subset = y_val[val_subset_indices]

X_test_pca_subset = X_test_pca[test_subset_indices]
y_test_subset = y_test[test_subset_indices]

# Applying SMOTE to the reduced subset
smote = SMOTE()
X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_pca_subset, y_train_smote_subset)



from imblearn.over_sampling import RandomOverSampler
# Applying SMOTE to the reduced subset
smote = SMOTE()
X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train_pca_subset, y_train_smote_subset)
print(X_train_oversampled.shape, X_train_oversampled.shape)

y_train_oversampled.shape

Score, test_predictions = acc_score(X_train_oversampled, y_train_oversampled, X_val_pca, y_val,X_test_pca_subset,y_test_subset , models, classifiers)
Score
#Score, test_predictions = acc_score(X_tra, y_train_smote, X_val_scaled, y_val)
#Score
test_predictions

# Calculate accuracy for each position/index
accuracies = [np.mean(pred == 1) for pred in test_predictions]

# Calculate the average accuracy across all positions/indices
average_accuracy = np.mean(accuracies)

# Print the results
print("Accuracy for each position/index:", accuracies)
print("Average Prediction Accuracy:", average_accuracy)

Score

import numpy as np
random_indices2 = np.random.choice(X_train_scaled.shape[0], subset_size, replace=False)
X_train_subset2 = X_train_scaled[random_indices2]
y_train_subset2 = y_train[random_indices2]


#train_subset_indices2 = np.random.choice(X_train_.shape[0], subset_size, replace=False)
val_subset_indices2 = np.random.choice(X_val_scaled.shape[0], subset_size, replace=False)
test_subset_indices2 = np.random.choice(X_test_scaled.shape[0], subset_size, replace=False)

# Subset the datasets using the generated indices


X_val_subset = X_val_scaled[val_subset_indices2]
y_val_subset2 = y_val[val_subset_indices2]

X_test_subset = X_test_scaled[test_subset_indices2]
y_test_subset2 = y_test[test_subset_indices2]
smote = SMOTE()
X_train_oversampled2, y_train_oversampled2 = smote.fit_resample(X_train_subset2, y_train_subset2)
print(X_train_oversampled2.shape, X_train_oversampled2.shape)

print(X_train_oversampled2.shape)

logmodel = RandomForestClassifier(n_estimators=150, random_state=42)



best_score, best_recalls, best_precisions, best_f1_scores, best_auc_rocs, best_pr_aucs, best_test_accs, best_chromosome = generations(X_train_oversampled2, y_train_oversampled2, X_val_subset, y_val_subset2, X_test_subset, y_test_subset2,100, X_train_oversampled2.shape[1],                                    64, 0.20, 5, logmodel)

# Print the results
print("Best Chromosome:", best_chromosome)
print("Best Validation Accuracy:", best_score[-1])
print("Best Validation ROC AUC:", best_auc_rocs[-1])
print("Best Test Accuracy:", best_test_accs[-1])

best_features = select_top_features(best_chromosome, num_features=X_train_oversampled2.shape[1])

# Visualize the selected feature indices using a bar plot
plt.figure(figsize=(10, 6))
plt.bar(range(len(best_features)), best_features)
plt.xlabel('Feature Index')
plt.ylabel('Selected')
plt.title('Selected Features After Genetic Algorithm')
plt.show()

column_names = combined_df_encoded.columns

# Map the feature indices to column names
selected_feature_names = [column_names[idx] for idx in best_features]

# Print the selected feature names
print("Selected Feature Names:")
print(selected_feature_names)
top_n = 50
selected_feature_names_top = selected_feature_names[:top_n]
best_features_top = best_features[:top_n]

plt.figure(figsize=(10, 6))  # Adjust the figure size as needed

plt.bar(range(len(selected_feature_names_top)), best_features_top)
plt.xlabel('Feature Index')
plt.ylabel('Selected')
plt.title('Top 20 Selected Features After Genetic Algorithm')
plt.xticks(range(len(selected_feature_names_top)), selected_feature_names_top, rotation='vertical')
plt.tight_layout()
plt.show()

best_recalls

best_sensitivities

best_sensitivities
def plot_roc_curve(fpr, tpr, roc_auc):
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc='lower right')
    plt.show()



chromo_df_bc

best_score

